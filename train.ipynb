{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from core.model import SAnD\n",
    "from data.mimiciii import get_mimic_iii\n",
    "from utils.trainer import NeuralNetworkClassifier\n",
    "from pyhealth.tasks import mortality_prediction_mimic3_fn\n",
    "from pyhealth.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sequences, labels = zip(*data)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    max_num_visits = max(num_visits)\n",
    "\n",
    "    x = torch.zeros((num_patients, max_num_visits, len(freq_codes)), dtype=torch.float)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            for code in visit:\n",
    "                \"\"\"\n",
    "                TODO: 1. check if code is in freq_codes;\n",
    "                      2. obtain the code index using code2idx;\n",
    "                      3. set the correspoindg element in x to 1.\n",
    "                \"\"\"\n",
    "                if code in freq_codes:\n",
    "                    x[i_patient, j_visit, code2idx[code]] = 1\n",
    "\n",
    "                y[i_patient] = labels[i_patient]\n",
    "\n",
    "    masks = torch.sum(x, dim=-1) > 0\n",
    "\n",
    "    return x, masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, batch_size, shuffle=False):\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train: float, val: float, test: float):\n",
    "    err = 1e-5\n",
    "    if 1 - err < (train + val + test) < 1 + err == False:\n",
    "        raise Exception(f\"{train=} + {val=} + {test=} = {train+val+test}. Needs to be 1.\")\n",
    "    length = len(data)\n",
    "    end_train = int(len(data) * train)\n",
    "    end_val = int(len(data) * val) + end_train\n",
    "\n",
    "    return data[:end_train], data[end_train:end_val], data[end_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {}\n",
    "\n",
    "def tokenizer_helper(sample, key: str) -> np.array:\n",
    "    if key not in tokenizers:\n",
    "        alls =  {s for l in [sample[key][0] for sample in dataset.samples] for s in l}\n",
    "        tokenizers[key] = Tokenizer(list(alls))\n",
    "    tokenizer = tokenizers[key]\n",
    "    items = sample[key][0]\n",
    "    item_table = np.zeros(shape=(tokenizer.get_vocabulary_size()))\n",
    "    item_indicies = tokenizer.convert_tokens_to_indices(items)\n",
    "    item_table[item_indicies] = True\n",
    "    return item_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_prediction_mimic3_fn: 100%|██████████| 46520/46520 [00:00<00:00, 121588.35it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = get_mimic_iii().set_task(mortality_prediction_mimic3_fn)\n",
    "sample = dataset.samples[0]\n",
    "(tokenizer_helper(sample, \"drugs\"), tokenizer_helper(sample, \"conditions\"), tokenizer_helper(sample, \"procedures\"))\n",
    "n_tokens = sum(v.get_vocabulary_size() for v in tokenizers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In Hospital Mortality: Mortality prediction is vital during rapid triage and risk/severity assessment. In Hospital\n",
    "Mortality is defined as the outcome of whether a patient dies\n",
    "during the period of hospital admission or lives to be discharged. This problem is posed as a binary classification one\n",
    "where each data sample spans a 24-hour time window. True\n",
    "mortality labels were curated by comparing date of death\n",
    "(DOD) with hospital admission and discharge times. The\n",
    "mortality rate within the benchmark cohort is only 13%.\n",
    "\"\"\"\n",
    "in_feature = n_tokens\n",
    "n_heads = 32\n",
    "factor = 32\n",
    "num_class = 2\n",
    "num_layers = 6\n",
    "\n",
    "patients = len(dataset.patient_to_index)\n",
    "seq_len = max(len(v) for v in dataset.patient_to_index.values())\n",
    "new_dataset =torch.zeros((patients, seq_len, n_tokens,))\n",
    "new_labels = torch.zeros((patients,))\n",
    "\n",
    "i = 0\n",
    "for p_id, visits in dataset.patient_to_index.items():\n",
    "    for n_visit, sample_idx in enumerate(visits):\n",
    "        sample = dataset.samples[sample_idx]\n",
    "        sample_data = np.concatenate((tokenizer_helper(sample, \"drugs\"), tokenizer_helper(sample, \"conditions\"), tokenizer_helper(sample, \"procedures\")))\n",
    "        new_dataset[i][n_visit] = torch.tensor(sample_data)\n",
    "        new_labels[i] = max(new_labels[i], sample[\"label\"])\n",
    "    i += 1\n",
    "# create dataloaders (they are <torch.data.DataLoader> object)\n",
    "train_data, val_data, test_data = split_data(new_dataset, 0.8, .1, .1)\n",
    "train_labels, val_labels, test_labels = split_data(new_labels, 0.8, .1, .1)\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = get_dataloader(train_dataset, batch_size=seq_len, shuffle=True)\n",
    "val_loader = get_dataloader(val_dataset, batch_size=seq_len, shuffle=False)\n",
    "test_loader = get_dataloader(test_dataset, batch_size=seq_len, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/samdoud/general/84247acd0256413d80b0637f72131f73\n",
      "\n",
      "  0%|          | 0/4948 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[39m=\u001b[39m NeuralNetworkClassifier(\n\u001b[0;32m      2\u001b[0m     SAnD(in_feature, seq_len, n_heads, factor, num_class, num_layers),\n\u001b[0;32m      3\u001b[0m     nn\u001b[39m.\u001b[39mCrossEntropyLoss(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     )\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[39m# training network\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     15\u001b[0m     {\n\u001b[0;32m     16\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: train_loader,\n\u001b[0;32m     17\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m: val_loader\n\u001b[0;32m     18\u001b[0m     },\n\u001b[0;32m     19\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[39m# evaluating\u001b[39;00m\n\u001b[0;32m     23\u001b[0m clf\u001b[39m.\u001b[39mevaluate(test_loader)\n",
      "File \u001b[1;32mc:\\Users\\Samuel\\Desktop\\SAnD\\utils\\trainer.py:162\u001b[0m, in \u001b[0;36mNeuralNetworkClassifier.fit\u001b[1;34m(self, loader, epochs, checkpoint_path, validation)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    161\u001b[0m pbar \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(total\u001b[39m=\u001b[39mlen_of_train_dataset)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m loader[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    163\u001b[0m     b_size \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    164\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Samuel\\.conda\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Samuel\\.conda\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Samuel\\.conda\\envs\\py38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate_fn\u001b[39m(data):\n\u001b[0;32m      2\u001b[0m     sequences, labels \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdata)\n\u001b[1;32m----> 3\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(labels, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\n\u001b[0;32m      4\u001b[0m     num_patients \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sequences)\n\u001b[0;32m      5\u001b[0m     num_visits \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(patient) \u001b[39mfor\u001b[39;00m patient \u001b[39min\u001b[39;00m sequences]\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "clf = NeuralNetworkClassifier(\n",
    "    SAnD(in_feature, seq_len, n_heads, factor, num_class, num_layers),\n",
    "    nn.CrossEntropyLoss(),\n",
    "    optim.Adam, optimizer_config={\n",
    "        \"lr\": 1e-5, \"betas\": (0.9, 0.98), \"eps\": 4e-09, \"weight_decay\": 5e-4},\n",
    "    experiment=Experiment(\n",
    "        api_key=\"eQ3INeSsFGUYKahSdEtjhry42\",\n",
    "        project_name=\"general\",\n",
    "        workspace=\"samdoud\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# training network\n",
    "clf.fit(\n",
    "    {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader\n",
    "    },\n",
    "    epochs=1\n",
    ")\n",
    "\n",
    "# evaluating\n",
    "clf.evaluate(test_loader)\n",
    "\n",
    "# save\n",
    "clf.save_to_file(\"./save_params/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
